<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Praveen Tirupattur</title>

  <meta name="author" content="Praveen Tirupattur">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.0.7/css/all.css">

</head>

<body>
  <table
    style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:63%;vertical-align:middle">
                  <p class="name" style="text-align: center;">
                    Praveen Tirupattur
                  </p>
                  <p>
                    O-1 visa holder, awarded for demonstrated extraordinary ability in computer vision and machine learning. Graduated with a Ph.D. in computer science, from the 
                      <a href="https://www.crcv.ucf.edu/">Center for Research in
                      Computer Vision</a>, <a href="https://www.ucf.edu/">UCF</a>, under the guidance of <a
                      href="https://www.crcv.ucf.edu/person/mubarak-shah/">Prof. Mubarak Shah.</a>
                    My research interests span various domains within computer vision and machine learning.
                    During my doctoral studies, I have focused extensively on tackling diverse challenges in video comprehension using supervised, 
                    weakly supervised, self-supervised, and zero-shot learning. This includes tasks such as action detection, temporal action localization,
                     and complex activity recognition. I have also worked on anomaly detection, gait recognition, person-Reid, and video understanding using large language models. Experienced in working with deep learning frameworks such as PyTorch, Keras, and Tensorflow.

                  </p>
                  <p style="text-align:center">
                    <a href="mailto:praveentirupattur@gmail.com">Email</a> &nbsp;/&nbsp;
                    <a href="data/PraveenTirupattur_Resume.pdf">CV</a> &nbsp;/&nbsp;
                    <a href="https://scholar.google.com/citations?user=zA7RnbUAAAAJ&hl=en">Scholar</a> &nbsp;/&nbsp;
                    <a href="https://twitter.com/ptirupat">Twitter</a> &nbsp;/&nbsp;
                    <a href="https://www.linkedin.com/in/praveen-tirupattur-2044ba51/">LinkedIn</a> &nbsp;/&nbsp;
                    <a href="https://github.com/ptirupat">Github</a>
                  </p>
                </td>
                <td style="padding:2.5%;width:40%;max-width:40%">
                  <a href="./images/praveen.jpg"><img
                      style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo"
                      src="./images/praveen.jpg" class="hoverZoomLink"></a>
                </td>
              </tr>
            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <h2>Updates</h2>
                  <p>
                    &nbsp <i class="fa fa-share-alt" style="font-size:12px"></i> Jan 2026: U.S. O-1A Extraordinary Ability Visa approved for work in Artificial Intelligence and Computer Vision. </font><br>
                    &nbsp <i class="fa fa-share-alt" style="font-size:12px"></i> May 2025: Paper accepted to ICML 2025 as an <font color="red">Oral (Top 1%)</font><br>
                    &nbsp <i class="fa fa-share-alt" style="font-size:12px"></i> September 2024: Joined <a href="https://www.photoday.com/"> PhotoDay </a> as a Machine Learning Researcher <br>
                    &nbsp <i class="fa fa-share-alt" style="font-size:12px"></i> August 2024: Graduate with Ph.D in Computer Science, from UCF  <br>
                    &nbsp <i class="fa fa-share-alt" style="font-size:12px"></i> June 2024: Successfully defended my Ph.D dissertation <br>
                    &nbsp <i class="fa fa-share-alt" style="font-size:12px"></i> December 2023: Paper accepted to AAAI 2024 <br>
                    &nbsp <i class="fa fa-share-alt" style="font-size:12px"></i> May 2023: Started summer internship at Amazon <br>
                    &nbsp <i class="fa fa-share-alt" style="font-size:12px"></i> October 2022: Patent granted for real-time spatio-temporal activity detection from untrimmed videos<br>
                    &nbsp <i class="fa fa-share-alt" style="font-size:12px"></i> May 2022: Started summer internship at Pinterest <br>
                    &nbsp <i class="fa fa-share-alt" style="font-size:12px"></i> March 2021: Paper accepted to CVPR 2021 as an <font color="red">Oral</font><br>
                    &nbsp <i class="fa fa-share-alt" style="font-size:12px"></i> January 2021: Our Gabriella paper has been awarded the <font color="red">best scientific paper</font> award at ICPR 2020<br>
                    &nbsp <i class="fa fa-share-alt" style="font-size:12px"></i> June 2020: Placed first at ActEV SDL Challenge (ActivityNet workshop at CVPR 2020)<br>
                    &nbsp <i class="fa fa-share-alt" style="font-size:12px"></i> October 2019: Placed second at the TRECVID leaderboard<br>
                    &nbsp <i class="fa fa-share-alt" style="font-size:12px"></i> August 2018: Paper accepted to ACM MM 2018<br>
                  </p>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <h2>Research</h2>
                </td>
              </tr>
            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:10px;width:25%;vertical-align:top">
                  <img src="images/mgd3_icml2025.png" alt="mgd3-icml2025" width="160"
                    style="border-style: none">
                </td>
                <td style="padding-bottom:10px;width:75%;vertical-align:middle">
                  <a href="https://openreview.net/forum?id=NIe74CY9lk" id="MCG_journal">
                    <papertitle>MGD <sup>3</sup>: Mode-Guided Dataset Distillation using Diffusion Models</papertitle>
                  </a>
                  <br>
                  <a href="https://www.linkedin.com/in/jachansantiago/">Jeffrey A Chan Santiago</a>, 
                  <strong>Praveen Tirupattur</strong>, <a href="https://sites.google.com/view/gauravnayak/"> Gaurav Kumar Nayak </a>,
                  <a href="https://scholar.google.com/citations?user=NIv_aeQAAAAJ&hl=en"> Gaowen Liu </a>,
                  <a href="https://www.crcv.ucf.edu/person/mubarak-shah/">Mubarak Shah</a>
                  <br>
                  <em>ICML</em>, 2025 <font color="red">(Oral; in top 1%)</font>
                  <br>
                  <a href="https://arxiv.org/html/2505.18963v1/">arxiv</a> /
                  <a href="https://github.com/jachansantiago/mode_guidance/">code</a> /
                  <a href="https://jachansantiago.com/mode-guided-distillation/">project page</a>
                  <p> We present a novel dataset distillation approach that uses a pre-trained diffusion model to generate high-quality synthetic datasets â€” without any retraining or fine-tuning. Our approach avoids expensive model retraining, saving computation; delivers better diversity and representativeness than existing methods; matches or exceeds SOTA performance on ImageNette, ImageIDC, ImageNet-100, and ImageNet-1K. </p>
                </td>
              </tr>
            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:10px;width:25%;vertical-align:top">
                  <img src="images/dvanet_aaai2024.png" alt="dvanet-aaai2024" width="160"
                    style="border-style: none">
                </td>
                <td style="padding-bottom:10px;width:75%;vertical-align:middle">
                  <a href="https://arxiv.org/pdf/2312.05719.pdf" id="MCG_journal">
                    <papertitle>DVANet: Disentangling View and Action Features for Multi-View Action Recognition</papertitle>
                  </a>
                  <br>
                  <a href="https://www.linkedin.com/in/nyle-siddiqui-b57828198/">Nyle Siddiqui</a>, 
                  <strong>Praveen Tirupattur</strong>,
                  <a href="https://www.crcv.ucf.edu/person/mubarak-shah/">Mubarak Shah</a>
                  <br>
                  <em>AAAI</em>, 2024
                  <br>
                  <a href="https://arxiv.org/abs/2312.05719">arxiv</a> /
                  <a href="https://github.com/NyleSiddiqui/MultiView_Actions">code</a> /
                  <a href="data/dvanet_aaai2023.bib">bibtex</a> /
                  <a href="https://nylesiddiqui.github.io/DVANet_webpage">project page</a>
                  <p> This study introduces a novel approach to multi-view action recognition, emphasizing the separation of learned action representations from view-specific details in videos. Addressing challenges like background variations and visibility differences in multiple viewpoints, we propose a unique configuration of learnable transformer decoder queries with two supervised contrastive losses. Our model outperforms all uni-modal counterparts significantly on four different datasets.</p>
                </td>
              </tr>
            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:10px;width:25%;vertical-align:top">
                  <img src="images/vad_cvpr2022.png" alt="vad-cvpr2022" width="160"
                    style="border-style: none">
                </td>
                <td style="padding-bottom:10px;width:75%;vertical-align:middle">
                  <a href="https://openaccess.thecvf.com/content/CVPR2022W/VDU/papers/Modi_Video_Action_Detection_Analysing_Limitations_and_Challenges_CVPRW_2022_paper.pdf" id="MCG_journal">
                    <papertitle>Video action detection: Analysing limitations and challenges</papertitle>
                  </a>
                  <br>
                  <a href="https://www.linkedin.com/in/rajat-modi-54377877/">Rajat Modi</a>, 
                  <a href="https://aayushjr.github.io/">Aayush Jung Rana</a>,
                  <a href="https://www.linkedin.com/in/akash-kumar-498600113/">Akash Kumar</a>,
                  <strong>Praveen Tirupattur</strong>, 
                  <a href="https://mse.ucf.edu/person/shrutivyas/">Shruti Vyas</a>, 
                  <a href="https://www.crcv.ucf.edu/person/rawat/">Yogesh S Rawat</a>, 
                  <a href="https://www.crcv.ucf.edu/person/mubarak-shah/">Mubarak Shah</a>
                  <br>
                  <em>CVPR</em>, 2022 
                  <br>
                  <a href="https://arxiv.org/pdf/2204.07892.pdf">arxiv</a> /
                  <a href="https://www.crcv.ucf.edu/research/projects/mama-multi-actor-multiaction-dataset-for-action-detection/">code</a> /
                  <a href="data/vad_cvpr2022.bib">bibtex</a> 
                  <p> Our work delves into attributes measuring dataset quality for video action detection, probing existing datasets' limitations and proposing the Multi Actor Multi Action (MAMA) dataset, addressing real-world application needs. We conduct a biasness study examining the temporal aspect's significance, questioning assumptions on temporal ordering's importance, revealing biases despite meticulous modeling.</p>
                </td>
              </tr>
            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:10px;width:25%;vertical-align:top">
                  <img src="images/mlad_cvpr2021.png" alt="mlad-cvpr2021" width="160" style="border-style: none">
                </td>
                <td style="padding-bottom:10px;width:75%;vertical-align:middle">
                  <a href="https://openaccess.thecvf.com/content/CVPR2021/html/Tirupattur_Modeling_Multi-Label_Action_Dependencies_for_Temporal_Action_Localization_CVPR_2021_paper.html"
                    id="MCG_journal">
                    <papertitle>Modeling Multi-Label Action Dependencies for Temporal Action Localization</papertitle>
                  </a>
                  <br>
                  <strong>Praveen Tirupattur</strong>,
                  <a href="https://www.linkedin.com/in/kevin-duarte-vision/">Kevin Duarte</a>,
                  <a href="https://www.crcv.ucf.edu/person/rawat/">Yogesh S Rawat</a>,
                  <a href="https://www.crcv.ucf.edu/person/mubarak-shah/">Mubarak Shah</a>
                  <br>
                  <em>CVPR</em>, 2021 <font color="red">(Oral presentation; in top 2.5%)</font>
                  <br>
                  <a href="https://arxiv.org/pdf/2103.03027.pdf">arxiv</a> /
                  <a href="https://github.com/ptirupat/MLAD">
                    code</a> /
                  <a href="data/mlad_cvpr2021.bib">bibtex</a> /
                  <a href="data/CVPR2021_MLAD_Final.pdf">slides</a>
                  /
                  <a href="https://www.crcv.ucf.edu/wp-content/uploads/2018/11/CVPR-2021-MLAD-Conference.mp4">video</a>
                  <p> We propose an attention-based architecture to capture action relationships in the context of
                    temporal action localization within untrimmed videos. Our approach discerns between relationships
                    among actions unfolding simultaneously and those occurring at different time steps, labeling them as
                    distinct action dependencies. To enhance action localization performance, we introduce a novel
                    Multi-Label Action Dependency (MLAD) layer, leveraging attention mechanisms to model these intricate
                    dependencies.</p>
                </td>
              </tr>
            </tbody>
          </table>


          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:10px;width:25%;vertical-align:top">
                  <img src="images/tinyactions_cvpr21.png" alt="tinyactions_cvpr21" width="160"
                    style="border-style: none">
                </td>
                <td style="padding-bottom:10px;width:75%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2107.11494" id="MCG_journal">
                    <papertitle>TinyAction Challenge: Recognizing Real-world Low-resolution Activities in Videos
                    </papertitle>
                  </a>
                  <br>
                  <strong>Praveen Tirupattur</strong>,
                  <a href="https://aayushjr.github.io/">Aayush Jung Rana</a>,
                  <a href="https://www.linkedin.com/in/tusharsangam/">Tushar Sangam</a>,
                  <a href="https://mse.ucf.edu/person/shrutivyas/">Shruti Vyas</a>,
                  <a href="https://www.crcv.ucf.edu/person/rawat/">Yogesh S Rawat</a>,
                  <a href="https://www.crcv.ucf.edu/person/mubarak-shah/">Mubarak Shah</a>
                  <br>
                  <em>CVPR</em>, 2021
                  <br>
                  <a href="https://arxiv.org/pdf/2107.11494.pdf">arxiv</a> /
                  <a href="https://www.crcv.ucf.edu/tiny-actions-challenge-cvpr2021/data/TinyVIRAT-v2.zip">
                    dataset</a> /
                  <a href="data/tinyactions_cvpr2021.bib">bibtex</a> /
                  <a href="https://www.crcv.ucf.edu/tiny-actions-challenge-cvpr2021">web page</a>
                  <p>
                    This paper outlines the TinyAction Challenge held at CVPR 2021, focusing on recognizing real-world
                    low-resolution activities in security videos. It introduces the benchmark dataset TinyVIRAT-v2, an
                    extension of TinyVIRAT, featuring naturally occurring low-resolution actions from security videos.
                    The challenge aims to address the difficulty of action recognition in tiny regions, providing a
                    benchmark for state-of-the-art methods.
                  </p>
                </td>
              </tr>
            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:10px;width:25%;vertical-align:top">
                  <img src="images/gabriella_icpr2020.png" alt="gabriella-icpr2020" width="160"
                    style="border-style: none">
                </td>
                <td style="padding-bottom:10px;width:75%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2004.11475" id="MCG_journal">
                    <papertitle>Gabriella: An Online System for Real-Time Activity Detection in Untrimmed Security
                      Videos</papertitle>
                  </a>
                  <br>
                  <a href="https://www.linkedin.com/in/mamshad-nayeem-rizve/">Mamshad Nayeem Rizve</a>, <a
                    href="https://www.linkedin.com/in/demiru/">Ugur Demir</a>, <strong>Praveen Tirupattur</strong>, <a
                    href="https://aayushjr.github.io/">Aayush Jung Rana</a>, <a
                    href="https://www.linkedin.com/in/kevin-duarte-vision/">Kevin Duarte</a>, <a
                    href="https://www.linkedin.com/in/ishan-dave-crcv">Ishan Dave</a>, <a
                    href="https://www.crcv.ucf.edu/person/rawat/">Yogesh S Rawat</a>, <a
                    href="https://www.crcv.ucf.edu/person/mubarak-shah/">Mubarak Shah</a>
                  <br>
                  <em>ICPR</em>, 2020 <font color="red">(Best paper award)</font>
                  <br>
                  <a href="https://arxiv.org/abs/2004.11475">arxiv</a> /
                  <a
                    href="https://www.crcv.ucf.edu/research/projects/gabriella-an-online-system-for-real-time-activity-detection-in-untrimmed-security-videos/">project
                    page</a> /
                  <a href="data/gabriella_icpr2020.bib">bibtex</a> /
                  <a href="https://www.crcv.ucf.edu/wp-content/uploads/2020/08/Projects_Gabriella_Slides.pdf">slides</a>
                  /
                  <a href="https://www.youtube.com/watch?v=O64331jZczo">video</a>
                  <p>Gabriella consists of three stages: tubelet extraction, activity classification, and online
                    tubelet merging. Gabriella utilizes a localization network for tubelet extraction, with a novel
                    Patch-Dice loss to handle variations in actor size, and a Tubelet-Merge Action-Split (TMAS)
                    algorithm to detect activities efficiently and robustly.</p>
                </td>
              </tr>
            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:10px;width:25%;vertical-align:top">
                  <img src="images/thoughtviz_acm18.png" alt="thoughtviz-acm2018" width="160"
                    style="border-style: none">
                </td>
                <td style="padding-bottom:10px;width:75%;vertical-align:middle">
                  <a href="https://dl.acm.org/doi/abs/10.1145/3240508.3240641" id="MCG_journal">
                    <papertitle>ThoughtViz: Visualizing Human Thoughts Using Generative Adversarial Network</papertitle>
                  </a>
                  <br>
                  <strong>Praveen Tirupattur</strong>,
                  <a href="https://www.crcv.ucf.edu/person/rawat/">Yogesh S Rawat</a>,
                  <a href="https://scholar.google.com/citations?user=Xc2rx8j4O7UC&hl=en&oi=ao">Concetto Spampinato</a>,
                  <a href="https://www.crcv.ucf.edu/person/mubarak-shah/">Mubarak Shah</a>
                  <br>
                  <em>ACM MM</em>, 2018
                  <br>
                  <a href="https://github.com/ptirupat/ThoughtViz">
                    code</a> /
                  <a href="data/thoughtviz_acm18.bib">bibtex</a> /
                  <a href="data/ACM_MM_18_Poster.pdf">poster</a>
                  <p> This paper explores decoding and visualizing human thoughts through Brain Computer Interface (BCI)
                    research.
                    Using ElectroEncephaloGram (EEG) signals, the proposed conditional Generative Adversarial Network
                    (GAN) effectively synthesizes visual representations of specific thoughts, such as digits,
                    characters, or objects.
                    The study showcases the potential of extracting meaningful visualizations from limited EEG data,
                    demonstrating the explicit encoding of thoughts in brain signals for semantically relevant image
                    generation.</p>
                </td>
              </tr>
            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <h2>Patents</h2>
                </td>
              </tr>
            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:10px;width:25%;vertical-align:top">
                  <img src="images/gabriella_patent.png" alt="gabriella-patent" width="160" style="border-style: none">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://patentimages.storage.googleapis.com/1a/1e/ec/af681095ca1677/US20220222940A1.pdf"
                    id="MCG_journal">
                    <papertitle>Methods of Real-Time Spatio-Temporal Activity Detection and Categorization from
                      Untrimmed Video Segments</papertitle>
                  </a>
                  <br>
                  <a href="https://www.crcv.ucf.edu/person/rawat/">Yogesh S Rawat</a>, <a
                    href="https://www.crcv.ucf.edu/person/mubarak-shah/">Mubarak Shah</a>, <a
                    href="https://aayushjr.github.io/">Aayush Jung Rana</a>,
                  <strong> Praveen Tirupattur </strong>,
                  <a href="https://www.linkedin.com/in/mamshad-nayeem-rizve/">Mamshad Nayeem Rizve</a>
                  <br>
                  US Patent 11468676
                  <br>
                  <a
                    href="https://patentimages.storage.googleapis.com/1a/1e/ec/af681095ca1677/US20220222940A1.pdf">Details</a>
                  <p></p>
                </td>
              </tr>

            </tbody>
          </table>

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody>
              <tr>
                <td>
                  <h2>Internships</h2>
                </td>
              </tr>
          </table>
          <table width="100%" align="center" border="0" cellpadding="20">
    </tbody>

    <tr>
      <td style="padding:10px;width:25%;vertical-align:top">
        <img src='images/amazon_logo.jpeg' style="width:100%;">
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <strong>Research Scientist Intern</strong>
        <br> Amazon Inc., Palo Alto, California, USA. May 2023- Nov 2023
        <br> Mentor: <a href="https://www.linkedin.com/in/jayakrishnan-unnikrishnan-86b14162/">Jay Krishnan</a>
        <p></p>
        <p> Worked on representation learning for long-form video understanding with vision-language training. Explored
          the idea of leveraging pre-trained Large Language Models (LLMs) to improve temporal understanding
          of video models.</p>
      </td>
    </tr>
    </tr>
  </table>
  <table width="100%" align="center" border="0" cellpadding="20">
    </tbody>

    <tr>
      <td style="padding:10px;width:25%;vertical-align:top">
        <img src='images/pinterest_logo.jpeg' style="width:100%;">
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <strong>Research Scientist Intern</strong>
        <br> Pinterest Inc., Remote, USA. May 2022 - Aug 2022
        <br> Mentor: <a href="https://www.linkedin.com/in/hao-yu-wu-39a75857/">Rex Wu</a>
        <p></p>
        <p> Worked on building a unified model for both image and video representation learning. Explored large-scale
          self-supervised training to learn representations for multiple visual modalities. Obtained improved
          performance over the in-house image-based model using the multi-modal training. </p>
      </td>
    </tr>
    </tr>
  </table>
  <br>
  <br>

  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tbody>
      <tr>
        <h3>Achievements and Awards</h3>
      </tr>
      <td style="padding:20px;width:25%;text-align:center;vertical-align:middle"><img src="images/trophy.png"
          width="50%" alt="Trophy Image"></td>

      <td width="75%" valign="center">
        <!-- 2nd Prize 2021 -->
        <div>
          <a href="https://www-nlpir.nist.gov/projects/tvpubs/tv21.slides/tv21.actev.slides.pdf">
            <strong>2<sup>nd</sup> place,</strong> 2021 -
            NIST TRECVID ActEV: Activities in Extended Video
          </a>
        </div>
        <br>
        <!-- 1st Prize 2021 PMiss@0.02tfa -->
        <div>
          <!-- <a href="#"> -->
          <strong>1<sup>st</sup> place,</strong> 2021 -
          PMiss@0.02tfa, ActivityNet ActEV SDL (<strong>CVPR</strong>)
          <!-- 		        </a> -->
        </div>
        <br>
        <!-- 1st Prize 2020 PMiss and nAUDC -->
        <div>
          <!-- 		        <a href="#"> -->
          <strong>1<sup>st</sup> place,</strong> 2020 -
          PMiss and nAUDC, ActivityNet ActEV SDL (<strong>CVPR</strong>)
          <!-- 		        </a> -->
        </div>
        <br>
        <!-- ASAPS -->
        <div>
          <a href="https://www.nist.gov/video/asaps-contest-1-winner-team-ucf-cmu"> 
            <strong>Winner Topic-4,</strong> 2020 -
            NIST ASAPS Challenge, Contest-1
          </a>
        </div>
        <br>
        <!-- Best Paper Award ICPR -->
        <div>
          <!-- 		        <a href="#"> -->
          <strong>Best Paper Award,</strong> 2020 -
          International Conference on Pattern Recognition (<strong> ICPR </strong>)
          <!-- 		        </a> -->
        </div>
        <br>
        <!-- 2nd Prize 2019 TRECVID -->
        <div>
          <a href="https://actev.nist.gov/trecvid19">
            <strong>2<sup>nd</sup> place,</strong> 2019 -
            TRECVID ActEV: Activities in Extended Video
          </a>
          <!-- 		        </a> -->
        </div>
        <br>
        <!-- ORCGS Doctoral Fellowship 2019-2020 -->
        <div>
          <!-- 		        <a href="#"> -->
          <strong>ORCGS Doctoral Fellowship,</strong> 2017 - University of Central Florida
          <!-- 		        </a> -->
        </div>
        <br>
      </td>
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tbody>
          <tr>
            <td>
              <h3>Professional Activities</h3>
            </td>
          </tr>
        </tbody>
      </table>
      <table width="100%" align="center" border="0" cellpadding="20">
        <tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/cvf.jpeg"></td>
            <td width="75%" valign="center">
              <a href="https://tinyactions-cvpr22.github.io/#organizers">Organizer, Tiny Actions Workshop (CVPR
                2022)</a><br>
              <a href="https://www.crcv.ucf.edu/tiny-actions-challenge-cvpr2021/">Organizer, Tiny Actions Workshop (CVPR
                2021)</a><br>

              <a href="https://cvpr2022.thecvf.com/">Reviewer, CVPR 2024, 2023, 2022</a><br>
              <a href="https://iccv2023.thecvf.com/">Reviewer, ICCV 2023</a><br>
              <a href="https://eccv2022.ecva.net/">Reviewer, ECCV 2022</a><br>
              <a href="https://iitjammu.ac.in/cvip2023/">Reviewer, CVIP 2022, 2023</a><br>
              <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=83">Reviewer, IEEE Transaction
                on Image Processing</a><br>
              <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=6046">Reviewer, IEEE
                Transactions on Multimedia</a><br>
              <a href="https://link.springer.com/journal/138">Reviewer, Machine Vision and Applications </a><br>
            </td>
          </tr>
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody>
              <tr>
                <td>
                  <h3>Mentor in NSF-REU</h3>
                </td>
              </tr>
            </tbody>
          </table>
          <table width="100%" align="center" border="0" cellpadding="20">
            <tbody>
              <tr>
                <td style="padding:20px;width:25%;text-align:center;vertical-align:middle"><img src="images/NSF_svg.png"
                    width="50%" alt="NSF Image"></td>
                <td width="75%" valign="center">
                  <a href="https://www.crcv.ucf.edu/nsf-projects/reu/reu-2019/">Urvi Gianchandani, REU 2019</a>
                  <br>
                  <a href="https://www.crcv.ucf.edu/nsf-projects/reu/reu-2020/">Pedro Contipelli, REU 2020</a>
                  <br>
                  <a href="https://www.crcv.ucf.edu/nsf-projects/reu/reu-2021/">Li Miao, REU 2021</a>
                  <br>
                  <a href="https://www.crcv.ucf.edu/nsf-projects/reu/reu-2024/">Keerthi Veeramachaneni, REU 2024</a>
                </td>
              </tr>
            </tbody>
          </table>

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody>
              <tr>
                <td>
                  <h3>Page Visits</h3>
                </td>
              </tr>
            </tbody>
          </table>

          <table width="100%" align="center" border="0" cellpadding="20">
            <tbody>
              <tr>
                <td style="padding:20px;width:25%;text-align:center;vertical-align:middle">
                  <script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=a&t=tt&d=BUAiPrxlg4zeiEfkdxZWDOaDzHZ_rqeUaLP5OnJ3VmQ'></script>
                </td>
              </tr>
            </tbody>
          </table>
          
          </td>
          </tr>
      </table>
      </body>

</html>
