<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Praveen Tirupattur</title>

  <meta name="author" content="Praveen Tirupattur">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
  <link rel="stylesheet" type="text/css" href="stylesheet.css">

</head>

<body>
  <table
    style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:63%;vertical-align:middle">
                  <p class="name" style="text-align: center;">
                    Praveen Tirupattur
                  </p>
                  <p>
                    Currently, I am pursuing my Ph.D. at the <a href="https://www.crcv.ucf.edu/">Center for Research in
                      Computer Vision</a>, <a href="https://www.ucf.edu/">UCF</a>, under the guidance of <a
                      href="https://www.crcv.ucf.edu/person/mubarak-shah/">Prof. Mubarak Shah.</a>
                    My research interests span various domains within computer vision and machine learning.
                    During my doctoral studies, I have focused extensively on tackling diverse challenges in video
                    comprehension using supervised, weakly supervised, self-supervised and zero-shot learning.
                    This includes tasks such as action detection, temporal action localization, and complex activity
                    recognition.
                    I have also worked on anomaly detection, gait recognition, person-reid, and on video understanding
                    using large lanuage models.

                  </p>
                  <p style="text-align:center">
                    <a href="mailto:praveentirupattur@gmail.com">Email</a> &nbsp;/&nbsp;
                    <a href="data/PraveenTirupattur_Resume.pdf">CV</a> &nbsp;/&nbsp;
                    <a href="https://scholar.google.com/citations?user=zA7RnbUAAAAJ&hl=en">Scholar</a> &nbsp;/&nbsp;
                    <a href="https://twitter.com/ptirupat">Twitter</a> &nbsp;/&nbsp;
                    <a href="https://www.linkedin.com/in/praveen-tirupattur-2044ba51/">LinkedIn</a> &nbsp;/&nbsp;
                    <a href="https://github.com/ptirupat">Github</a>
                  </p>
                </td>
                <td style="padding:2.5%;width:40%;max-width:40%">
                  <a href="./images/praveen.jpg"><img
                      style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo"
                      src="./images/praveen.jpg" class="hoverZoomLink"></a>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <h2>Research</h2>
                </td>
              </tr>
            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:10px;width:25%;vertical-align:top">
                  <img src="images/mlad_cvpr2021.png" alt="mlad-cvpr2021" width="160"
                    style="border-style: none">
                </td>
                <td style="padding-bottom:10px;width:75%;vertical-align:middle">
                  <a href="https://openaccess.thecvf.com/content/CVPR2021/html/Tirupattur_Modeling_Multi-Label_Action_Dependencies_for_Temporal_Action_Localization_CVPR_2021_paper.html" id="MCG_journal">
                    <papertitle>Modeling Multi-Label Action Dependencies for Temporal Action Localization</papertitle>
                  </a>
                  <br>
                  <strong>Praveen Tirupattur</strong>, 
                  <a href="https://www.linkedin.com/in/kevin-duarte-vision/">Kevin Duarte</a>, 
                  <a href="https://www.crcv.ucf.edu/person/rawat/">Yogesh S Rawat</a>, 
                  <a href="https://www.crcv.ucf.edu/person/mubarak-shah/">Mubarak Shah</a>
                  <br>
                  <em>CVPR</em>, 2021 <font color="red">(Oral)</font>
                  <br>
                  <a href="https://arxiv.org/pdf/2103.03027.pdf">arxiv</a> /
                  <a
                    href="https://github.com/ptirupat/MLAD">
                    code</a> /
                  <a href="data/mlad_cvpr2021.bib">bibtex</a> /
                  <a href="data/CVPR2021_MLAD_Final.pdf">slides</a>
                  /
                  <a href="https://www.crcv.ucf.edu/wp-content/uploads/2018/11/CVPR-2021-MLAD-Conference.mp4">video</a>
                  <p> We propose an attention-based architecture to capture action relationships in the context of temporal action localization within untrimmed videos. Our approach discerns between relationships among actions unfolding simultaneously and those occurring at different time steps, labeling them as distinct action dependencies. To enhance action localization performance, we introduce a novel Multi-Label Action Dependency (MLAD) layer, leveraging attention mechanisms to model these intricate dependencies.</p>
                </td>
              </tr>
            </tbody>
          </table>


          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:10px;width:25%;vertical-align:top">
                  <img src="images/tinyactions_cvpr21.png" alt="tinyactions_cvpr21" width="160"
                    style="border-style: none">
                </td>
                <td style="padding-bottom:10px;width:75%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2107.11494" id="MCG_journal">
                    <papertitle>TinyAction Challenge: Recognizing Real-world Low-resolution Activities in Videos</papertitle>
                  </a>
                  <br>
                  <strong>Praveen Tirupattur</strong>, 
                  <a href="https://aayushjr.github.io/">Aayush Jung Rana</a>, 
                  <a href="https://www.linkedin.com/in/tusharsangam/">Tushar Sangam</a>, 
                  <a href="https://mse.ucf.edu/person/shrutivyas/">Shruti Vyas</a>, 
                  <a href="https://www.crcv.ucf.edu/person/rawat/">Yogesh S Rawat</a>, 
                  <a href="https://www.crcv.ucf.edu/person/mubarak-shah/">Mubarak Shah</a>
                  <br>
                  <em>CVPR</em>, 2021 
                  <br>
                  <a href="https://arxiv.org/pdf/2107.11494.pdf">arxiv</a> /
                  <a
                    href="https://www.crcv.ucf.edu/tiny-actions-challenge-cvpr2021/data/TinyVIRAT-v2.zip">
                    dataset</a> /
                  <a href="data/tinyactions_cvpr2021.bib">bibtex</a> /
                  <a href="https://www.crcv.ucf.edu/tiny-actions-challenge-cvpr2021">web page</a>
                  <p> 
                  This paper outlines the TinyAction Challenge held at CVPR 2021, focusing on recognizing real-world low-resolution activities in security videos. It introduces the benchmark dataset TinyVIRAT-v2, an extension of TinyVIRAT, featuring naturally occurring low-resolution actions from security videos. 
                  The challenge aims to address the difficulty of action recognition in tiny regions, providing a benchmark for state-of-the-art methods.
                  </p>
                </td>
              </tr>
            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:10px;width:25%;vertical-align:top">
                  <img src="images/gabriella_icpr2020.png" alt="gabriella-icpr2020" width="160"
                    style="border-style: none">
                </td>
                <td style="padding-bottom:10px;width:75%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2004.11475" id="MCG_journal">
                    <papertitle>Gabriella: An Online System for Real-Time Activity Detection in Untrimmed Security
                      Videos</papertitle>
                  </a>
                  <br>
                  <a href="https://www.linkedin.com/in/mamshad-nayeem-rizve/">Mamshad Nayeem Rizve</a>, <a
                    href="https://www.linkedin.com/in/demiru/">Ugur Demir</a>, <strong>Praveen Tirupattur</strong>, <a
                    href="https://aayushjr.github.io/">Aayush Jung Rana</a>, <a
                    href="https://www.linkedin.com/in/kevin-duarte-vision/">Kevin Duarte</a>, <a
                    href="https://www.linkedin.com/in/ishan-dave-crcv">Ishan Dave</a>, <a
                    href="https://www.crcv.ucf.edu/person/rawat/">Yogesh S Rawat</a>, <a
                    href="https://www.crcv.ucf.edu/person/mubarak-shah/">Mubarak Shah</a>
                  <br>
                  <em>ICPR</em>, 2020 <font color="red">(Best paper award)</font>
                  <br>
                  <a href="https://arxiv.org/abs/2004.11475">arxiv</a> /
                  <a
                    href="https://www.crcv.ucf.edu/research/projects/gabriella-an-online-system-for-real-time-activity-detection-in-untrimmed-security-videos/">project
                    page</a> /
                  <a href="data/gabriella_icpr2020.bib">bibtex</a> /
                  <a href="https://www.crcv.ucf.edu/wp-content/uploads/2020/08/Projects_Gabriella_Slides.pdf">slides</a>
                  /
                  <a href="https://www.youtube.com/watch?v=O64331jZczo">video</a>
                  <p>Gabriella consists of three stages: tubelet extraction, activity classification, and online
                    tubelet merging. Gabriella utilizes a localization network for tubelet extraction, with a novel
                    Patch-Dice loss to handle variations in actor size, and a Tubelet-Merge Action-Split (TMAS)
                    algorithm to detect activities efficiently and robustly.</p>
                </td>
              </tr>
            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:10px;width:25%;vertical-align:top">
                  <img src="images/thoughtviz_acm18.png" alt="thoughtviz-acm2018" width="160"
                    style="border-style: none">
                </td>
                <td style="padding-bottom:10px;width:75%;vertical-align:middle">
                  <a href="https://dl.acm.org/doi/abs/10.1145/3240508.3240641" id="MCG_journal">
                    <papertitle>ThoughtViz: Visualizing Human Thoughts Using Generative Adversarial Network</papertitle>
                  </a>
                  <br>
                  <strong>Praveen Tirupattur</strong>, 
                  <a href="https://www.crcv.ucf.edu/person/rawat/">Yogesh S Rawat</a>, 
                  <a href="https://scholar.google.com/citations?user=Xc2rx8j4O7UC&hl=en&oi=ao">Concetto Spampinato</a>, 
                  <a href="https://www.crcv.ucf.edu/person/mubarak-shah/">Mubarak Shah</a>
                  <br>
                  <em>ACM MM</em>, 2018 
                  <br>
                  <a
                    href="https://github.com/ptirupat/ThoughtViz">
                    code</a> /
                  <a href="data/thoughtviz_acm18.bib">bibtex</a> /
                  <a href="data/ACM_MM_18_Poster.pdf">poster</a> 
                  <p> This paper explores decoding and visualizing human thoughts through Brain Computer Interface (BCI) research. 
                    Using ElectroEncephaloGram (EEG) signals, the proposed conditional Generative Adversarial Network (GAN) effectively synthesizes visual representations of specific thoughts, such as digits, characters, or objects. 
                    The study showcases the potential of extracting meaningful visualizations from limited EEG data, demonstrating the explicit encoding of thoughts in brain signals for semantically relevant image generation.</p>
                </td>
              </tr>
            </tbody>
          </table>
          
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <h2>Patents</h2>
                </td>
              </tr>
            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:10px;width:25%;vertical-align:top">
                  <img src="images/gabriella_patent.png" alt="gabriella-patent" width="160" style="border-style: none">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://patentimages.storage.googleapis.com/1a/1e/ec/af681095ca1677/US20220222940A1.pdf"
                    id="MCG_journal">
                    <papertitle>Methods of Real-Time Spatio-Temporal Activity Detection and Categorization from
                      Untrimmed Video Segments</papertitle>
                  </a>
                  <br>
                  <a href="https://www.crcv.ucf.edu/person/rawat/">Yogesh S Rawat</a>, <a
                    href="https://www.crcv.ucf.edu/person/mubarak-shah/">Mubarak Shah</a>, <a
                    href="https://aayushjr.github.io/">Aayush Jung Rana</a>, 
                    <strong> Praveen Tirupattur </strong>,
                  <a href="https://www.linkedin.com/in/mamshad-nayeem-rizve/">Mamshad Nayeem Rizve</a>
                  <br>
                  US Patent 11468676
                  <br>
                  <a
                    href="https://patentimages.storage.googleapis.com/1a/1e/ec/af681095ca1677/US20220222940A1.pdf">Details</a>
                  <p></p>
                </td>
              </tr>

            </tbody>
          </table>
         
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody>
              <tr>
                <td>
                  <h2>Internships</h2>
                </td>
              </tr>
          </table>
          <table width="100%" align="center" border="0" cellpadding="20">
    </tbody>

    <tr>
      <td style="padding:10px;width:25%;vertical-align:top">
        <img src='images/amazon_logo.jpeg' style="width:100%;">
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <strong>Research Scientist/ Engineer Intern</strong>
        <br> Amazon Inc., Palo Alto, California, USA. May 2023- Nov 2023
        <br> Mentor: <a href="https://www.linkedin.com/in/jayakrishnan-unnikrishnan-86b14162/">Jay Krishnan</a>
        <p></p>
        <p> Worked on representation learning for long-form video understanding with vision-language training. Explored the idea of leveraging pre-trained Large Language Models (LLMs) to improve temporal understanding
          of video models.</p>
      </td>
    </tr>
    </tr>
  </table>
  <table width="100%" align="center" border="0" cellpadding="20">
    </tbody>

    <tr>
      <td style="padding:10px;width:25%;vertical-align:top">
        <img src='images/pinterest_logo.jpeg' style="width:100%;">
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <strong>Research Scientist Intern</strong>
        <br> Pinterest Inc., Remote, USA. May 2022 - Aug 2022
        <br> Mentor: <a href="https://www.linkedin.com/in/hao-yu-wu-39a75857/">Rex Wu</a>
        <p></p>
        <p> Worked on building a unified model for both image and video representation learning. Explored large-scale self-supervised training to learn representations for multiple visual modalities. Obtained improved performance over the in-house image-based model using the multi-modal training. </p>
      </td>
    </tr>
    </tr>
  </table>
  <br>
  <br>
  
  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tbody>
      <tr>
          <h3>Achievements and Awards</h3>
      </tr>
      <td style="padding:20px;width:25%;text-align:center;vertical-align:middle"><img src="images/trophy.png"
          width="50%" alt="Trophy Image"></td>

      <td width="75%" valign="center">
        <!-- 2nd Prize 2021 -->
        <div>
          <a href="https://www-nlpir.nist.gov/projects/tvpubs/tv21.slides/tv21.actev.slides.pdf">
            <strong>2<sup>nd</sup> place,</strong> 2021 -
            NIST TRECVID ActEV: Activities in Extended Video
          </a>
        </div>
        <br>
        <!-- 1st Prize 2021 PMiss@0.02tfa -->
        <div>
          <!-- <a href="#"> -->
          <strong>1<sup>st</sup> place,</strong> 2021 -
          PMiss@0.02tfa, ActivityNet ActEV SDL (<strong>CVPR</strong>)
          <!-- 		        </a> -->
        </div>
        <br>
        <!-- 1st Prize 2020 PMiss and nAUDC -->
        <div>
          <!-- 		        <a href="#"> -->
          <strong>1<sup>st</sup> place,</strong> 2020 -
          PMiss and nAUDC, ActivityNet ActEV SDL (<strong>CVPR</strong>)
          <!-- 		        </a> -->
        </div>
        <br>
        <!-- ASAPS -->
        <div>
          <!-- 		        <a href="#"> -->
          <strong>Winner Topic-4,</strong> 2020 -
          NIST ASAPS Challenge, Contest-1 
          <!-- 		        </a> -->
        </div>
        <br>
        <!-- Best Paper Award ICPR -->
        <div>
          <!-- 		        <a href="#"> -->
          <strong>Best Paper Award,</strong> 2020 -
          International Conference on Pattern Recognition (<strong> ICPR </strong>)
          <!-- 		        </a> -->
        </div>
        <br>
        <!-- 2nd Prize 2019 TRECVID -->
        <div>
          <a href="https://actev.nist.gov/trecvid19">
          <strong>2<sup>nd</sup> place,</strong> 2019 -
          TRECVID ActEV: Activities in Extended Video
         </a>
          <!-- 		        </a> -->
        </div>
        <br>
        <!-- ORCGS Doctoral Fellowship 2019-2020 -->
        <div>
          <!-- 		        <a href="#"> -->
          <strong>ORCGS Doctoral Fellowship,</strong> 2017 - University of Central Florida
          <!-- 		        </a> -->
        </div>
        <br>
      </td>
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tbody>
          <tr>
            <td>
              <h3>Professional Activities</h3>
            </td>
          </tr>
        </tbody>
      </table>
      <table width="100%" align="center" border="0" cellpadding="20">
        <tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/cvf.jpeg"></td>
            <td width="75%" valign="center">
              <a href="https://tinyactions-cvpr22.github.io/#organizers">Organizer, Tiny Actions Workshop (CVPR 2022)</a><br>
              <a href="https://www.crcv.ucf.edu/tiny-actions-challenge-cvpr2021/">Organizer, Tiny Actions Workshop (CVPR 2021)</a><br>
              
              <a href="https://cvpr2022.thecvf.com/">Reviewer, CVPR 2024, 2023, 2022</a><br>
              <a href="https://iccv2023.thecvf.com/">Reviewer, ICCV 2023</a><br>
              <a href="https://eccv2022.ecva.net/">Reviewer, ECCV 2022</a><br>
              <a href="https://iitjammu.ac.in/cvip2023/">Reviewer, CVIP 2022, 2023</a><br>
              <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=83">Reviewer, IEEE Transaction
                on Image Processing</a><br>
              <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=6046">Reviewer, IEEE
                Transactions on Multimedia</a><br>
              <a href="https://link.springer.com/journal/138">Reviewer, Machine Vision and Applications </a><br>
            </td>
          </tr>
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody>
              <tr>
                <td>
                  <h3>Mentor in NSF-REU</h3>
                </td>
              </tr>
            </tbody>
          </table>
          <table width="100%" align="center" border="0" cellpadding="20">
            <tbody>
              <tr>
                <td style="padding:20px;width:25%;text-align:center;vertical-align:middle"><img src="images/NSF_svg.png"
                    width="50%" alt="NSF Image"></td>
                <td width="75%" valign="center">
                  <a href="https://www.crcv.ucf.edu/nsf-projects/reu/reu-2019/">Urvi Gianchandani, REU 2019</a>
                  <br>
                  <a href="https://www.crcv.ucf.edu/nsf-projects/reu/reu-2020/">Pedro Contipelli, REU 2020</a>
                  <br>
                  <a href="https://www.crcv.ucf.edu/nsf-projects/reu/reu-2021/">Li Miao, REU 2021</a>
                </td>
              </tr>
            </tbody>
          </table>
          </td>
          </tr>
      </table>
</body>

</html>